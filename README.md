````markdown
# ğŸ“š PDF Chatbot en EspaÃ±ol usando LangChain, Chroma, HuggingFace y Ollama

Este proyecto construye una API REST con FastAPI que permite realizar preguntas en espaÃ±ol sobre el contenido de un archivo PDF. Utiliza tÃ©cnicas modernas de NLP y recuperaciÃ³n de informaciÃ³n como **LangChain**, **Chroma**, **HuggingFace Embeddings** y un modelo LLM local con **Ollama** (ej. LLaMA 2).

## ğŸš€ CaracterÃ­sticas

- Carga y divide archivos PDF con LangChain.
- Genera y persiste una base vectorial con ChromaDB.
- Usa embeddings multilingÃ¼es para recuperar documentos relevantes.
- Integra un modelo local como LLaMA 2 vÃ­a Ollama para generar respuestas en espaÃ±ol.
- Expone una API REST para consultas.

---

## ğŸ“¦ Requisitos

- Python 3.10+
- [Ollama instalado](https://ollama.ai)
- Dependencias:

```bash
pip install langchain langchain-community langchain-huggingface langchain-chroma langchain-ollama fastapi uvicorn
````

---

## ğŸ§  Modelos y Archivos

* ğŸ“„ `manual.pdf`: documento fuente para preguntas (coloca el tuyo en el mismo directorio).
* ğŸ§  Embeddings: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`.
* ğŸ’¬ LLM: `llama2` (puede ser otro compatible con Ollama).

---

## ğŸ› ï¸ Estructura del CÃ³digo

### 1. IndexaciÃ³n y Embeddings

```python
loader = PyPDFLoader("manual.pdf")
documents = loader.load_and_split()
vectorstore = Chroma.from_documents(documents, embeddings, persist_directory="./chroma_db")
```

Si ya existe una base vectorial, esta se carga automÃ¡ticamente.

### 2. InicializaciÃ³n del LLM

```python
llm = OllamaLLM(model="llama2")
```

AsegÃºrate de tener el modelo descargado en tu entorno Ollama.

### 3. API REST

```python
@app.get("/chat")
def chat(query: str):
    docs = vectorstore.similarity_search(query, k=2)
    context_text = docs[0].page_content
    prompt = f"..."
    respuesta = llm.invoke(prompt)
    return {"respuesta": respuesta}
```

---

## â–¶ï¸ EjecuciÃ³n

```bash
python app.py
```

La API se ejecutarÃ¡ en: `http://localhost:8000/chat?query=tu_pregunta`

---

## ğŸ“¤ Ejemplo de Uso

```bash
curl "http://localhost:8000/chat?query=Â¿CuÃ¡l es el objetivo del manual?"
```

Respuesta:

```json
{
  "respuesta": "El objetivo del manual es..."
}
```

---

## ğŸ“ Directorios Generados

* `./chroma_db/`: contiene la base de datos vectorial persistente (`chroma.sqlite3`, etc).

---

## ğŸ§ª Recomendaciones

* Usa PDFs en espaÃ±ol para obtener respuestas mÃ¡s coherentes.
* El modelo `llama2` debe estar previamente descargado con:

```bash
ollama run llama2
```

---

## ğŸ“„ Licencia

MIT

---

## âœ¨ Autor

Desarrollado por [Tu Nombre o Usuario](https://github.com/tuusuario)

```

---

Â¿Te gustarÃ­a que tambiÃ©n genere el `requirements.txt` o un `Dockerfile` para desplegarlo fÃ¡cilmente?
```
